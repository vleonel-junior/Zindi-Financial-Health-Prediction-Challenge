# ğŸ† STRATÃ‰GIE GAGNANTE - Zindi Financial Health Challenge

## ğŸ“Š Analyse du ProblÃ¨me

### Dataset
- **Train**: 9,618 samples, 39 features
- **Test**: 2,405 samples
- **Taille idÃ©ale** pour TabPFN (<10K) + Gradient Boosting

### Distribution Target (DÃ‰SÃ‰QUILIBRE CRITIQUE!)
| Classe | Count | % |
|--------|-------|---|
| Low | 6,280 | 65.3% |
| Medium | 2,868 | 29.8% |
| **High** | **470** | **4.9%** |

âš ï¸ La classe **High** est trÃ¨s minoritaire = challenge pour le F1-score!

### Valeurs Manquantes (>20%)
26 colonnes avec >5% de manquants, dont:
- `uses_informal_lender`: 46.7%
- `uses_friends_family_savings`: 46.7%
- `motivation_make_more_money`: 44.6%
- etc.

---

## ğŸ”§ ProblÃ¨mes IdentifiÃ©s dans le Preprocessing Actuel

### 1. Mapping Trop Agressif
```python
# âŒ PROBLÃˆME: Perte d'information
"Yes, sometimes" â†’ "Yes"
"Never had" â†’ "No"
"Yes, always" â†’ "Yes"
```

**Solution**: Garder les nuances!
```python
# âœ… SOLUTION: Mapping conservateur
"Yes, sometimes" â†’ "sometimes"
"Yes, always" â†’ "always"
"Never had" â†’ "never"
"Used to have..." â†’ "used_to"
```

### 2. One-Hot Encoding Inutile
- Explose les dimensions
- Dilue l'information pour CatBoost/LGBM
- **Solution**: CatBoost gÃ¨re nativement les strings!

### 3. StandardScaler Inutile
- Les modÃ¨les arbres n'ont pas besoin de normalisation
- **Solution**: Retirer le scaler

### 4. Pas de Gestion du DÃ©sÃ©quilibre
- **Solution**: `class_weight='balanced'` + sample weights

---

## ğŸš€ STRATÃ‰GIE GAGNANTE

### Architecture Ensemble Optimale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ENSEMBLE ULTIME                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  CatBoost   â”‚  â”‚  LightGBM   â”‚  â”‚   XGBoost   â”‚    â”‚
â”‚   â”‚   ~30%      â”‚  â”‚    ~25%     â”‚  â”‚    ~20%     â”‚    â”‚
â”‚   â”‚             â”‚  â”‚             â”‚  â”‚             â”‚    â”‚
â”‚   â”‚ â€¢ Cat natif â”‚  â”‚ â€¢ Rapide    â”‚  â”‚ â€¢ Robuste   â”‚    â”‚
â”‚   â”‚ â€¢ Balanced  â”‚  â”‚ â€¢ Balanced  â”‚  â”‚ â€¢ Weighted  â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    TabPFN                        â”‚   â”‚
â”‚   â”‚                     ~25%                         â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚   â€¢ Zero-shot learning (pas d'overfitting!)     â”‚   â”‚
â”‚   â”‚   â€¢ GÃ¨re catÃ©gorielles nativement               â”‚   â”‚
â”‚   â”‚   â€¢ IdÃ©al pour <10K samples                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â”‚              â†“ Optimisation Poids â†“                     â”‚
â”‚                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚        Weighted Average + Threshold Tuning       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi Cette Architecture?

| ModÃ¨le | RÃ´le | Force |
|--------|------|-------|
| **CatBoost** | Principal | Meilleur sur catÃ©gorielles, natif |
| **LightGBM** | Feature importance | Rapide, bon pour sÃ©lection |
| **XGBoost** | DiversitÃ© | ComplÃ©mentaire, robuste |
| **TabPFN** | Zero-shot | Pas d'overfitting, diversitÃ© |

### Gestion du DÃ©sÃ©quilibre

```python
# CatBoost
auto_class_weights='Balanced'

# LightGBM
class_weight='balanced'

# XGBoost
sample_weight = [class_weight[yi] for yi in y_train]
```

---

## ğŸ“ Scripts CrÃ©Ã©s

### 1. `ultimate_ensemble.py` (RECOMMANDÃ‰)
- Ensemble complet avec TabPFN
- Optimisation automatique des poids
- 5-fold CV avec early stopping

```bash
python ultimate_ensemble.py
```

### 2. `quick_ensemble.py` (Version Rapide)
- Sans TabPFN
- HyperparamÃ¨tres prÃ©-optimisÃ©s
- Pour tests rapides

```bash
python quick_ensemble.py
```

### 3. `winning_strategy_pipeline.py` (Version ComplÃ¨te)
- Optimisation Optuna des hyperparamÃ¨tres
- Feature selection
- Threshold optimization

```bash
python winning_strategy_pipeline.py
```

---

## ğŸ¯ Optimisation F1-Score

### 1. Class Weights
```python
# Calcul automatique
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
```

### 2. Threshold Tuning (Optionnel)
Pour les classes dÃ©sÃ©quilibrÃ©es, ajuster les seuils:
```python
# Seuils inversement proportionnels aux frÃ©quences
thresholds = [proportion_class_i / sum(proportions)]
adjusted_proba = proba / thresholds
final_pred = adjusted_proba.argmax(axis=1)
```

### 3. Ensemble Weight Search
```python
# Grid search sur les poids
for w_cb in range(0.2, 0.6):
    for w_lgbm in range(0.1, 0.4):
        for w_xgb in range(0.1, 0.3):
            w_tabpfn = 1 - w_cb - w_lgbm - w_xgb
            # Calculer F1 et garder le meilleur
```

---

## ğŸ“ˆ RÃ©sultats Attendus

BasÃ© sur l'analyse TabArena 2025:

| Configuration | F1 Attendu |
|--------------|-----------|
| LGBM seul | ~0.87 |
| CatBoost seul | ~0.87-0.88 |
| Ensemble CB+LGBM+XGB | ~0.88-0.89 |
| **Ensemble + TabPFN** | **~0.89-0.91** |

---

## ğŸ”„ Workflow RecommandÃ©

1. **Test rapide**: `python quick_ensemble.py`
2. **Si TabPFN installÃ©**: `python ultimate_ensemble.py`
3. **Pour optimisation complÃ¨te**: `python winning_strategy_pipeline.py`

---

## âš ï¸ Points d'Attention

1. **TabPFN**: NÃ©cessite `pip install tabpfn` et authentification HuggingFace
2. **GPU**: CatBoost/XGBoost/TabPFN bÃ©nÃ©ficient du GPU
3. **MÃ©moire**: TabPFN peut Ãªtre gourmand en RAM pour les grands datasets

---

## ğŸ“ Checklist Avant Soumission

- [ ] VÃ©rifier la distribution des prÃ©dictions (pas trop biaisÃ©e)
- [ ] Comparer avec la distribution du train
- [ ] VÃ©rifier le format ID,Target
- [ ] Tester plusieurs seeds pour la stabilitÃ©
